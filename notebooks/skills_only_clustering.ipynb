{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Datasets/df_all_linkedin.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(stopWords, descriptions):\n",
    "    cleaned_descriptions = []\n",
    "    for description in descriptions:\n",
    "        temp_list = []\n",
    "        for word in description.split():\n",
    "            if word not in stopWords:\n",
    "                temp_list.append(word.lower())\n",
    "        cleaned_descriptions.append(' '.join(temp_list))\n",
    "    return np.array(cleaned_descriptions)\n",
    "\n",
    "def remove_punctuation(descriptions):\n",
    "    no_punct_descriptions = []\n",
    "    for description in descriptions:\n",
    "        description_no_punct = ' '.join(RegexpTokenizer(r'\\w+').tokenize(description))\n",
    "        no_punct_descriptions.append(description_no_punct)\n",
    "    return np.array(no_punct_descriptions)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {'J': wordnet.ADJ,\n",
    "               'N': wordnet.NOUN,\n",
    "               'V': wordnet.VERB,\n",
    "               'R': wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_descriptions(descriptions):\n",
    "    cleaned_descriptions = []\n",
    "    for description in descriptions:\n",
    "        temp_list = []\n",
    "        for word in description.split():\n",
    "            cleaned_word = WordNetLemmatizer().lemmatize(word, get_wordnet_pos(word))\n",
    "            temp_list.append(cleaned_word)\n",
    "        cleaned_descriptions.append(' '.join(temp_list))\n",
    "    return np.array(cleaned_descriptions)\n",
    "\n",
    "def clean_descriptions(stopWords, descriptions):\n",
    "    no_punct = remove_punctuation(descriptions)\n",
    "    no_punct_sw = remove_stopwords(stopWords, no_punct)\n",
    "    cleaned = lemmatize_descriptions(no_punct_sw)\n",
    "    return cleaned\n",
    "\n",
    "def get_representative_words(vectorizer, kmeans):\n",
    "    sorted_centroids = []\n",
    "    for cluster in kmeans.cluster_centers_:\n",
    "        top_10 = np.argsort(cluster)[::-1]\n",
    "        sorted_centroids.append(top_10[:10])\n",
    "    for idx, c in enumerate(sorted_centroids):\n",
    "        print(f'\\nCluster {idx}\\n')\n",
    "        for idx in c:\n",
    "            print(vectorizer.get_feature_names()[idx])\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic %d:' % (topic_idx))\n",
    "        print(' '.join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, get_wordnet_pos(text)))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in stopWords and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Datasets/df_all_linked.csv', index_col=0)\n",
    "descriptions = df['Description'].values\n",
    "for descrip in descriptions[:20]:\n",
    "    print('\\n Next Description:')\n",
    "    print(descrip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = {\n",
    "    'python', 'sql', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "\n",
      "product\n",
      "user\n",
      "feature\n",
      "system\n",
      "learn\n",
      "engineer\n",
      "distribute\n",
      "model\n",
      "security\n",
      "build\n",
      "\n",
      "Cluster 1\n",
      "\n",
      "model\n",
      "machine\n",
      "scientist\n",
      "learn\n",
      "insight\n",
      "job\n",
      "large\n",
      "solution\n",
      "com\n",
      "company\n",
      "\n",
      "Cluster 2\n",
      "\n",
      "position\n",
      "research\n",
      "energy\n",
      "model\n",
      "application\n",
      "system\n",
      "include\n",
      "must\n",
      "resume\n",
      "engineering\n",
      "\n",
      "Cluster 3\n",
      "\n",
      "state\n",
      "local\n",
      "ai\n",
      "client\n",
      "law\n",
      "federal\n",
      "analytics\n",
      "big\n",
      "engineering\n",
      "currently\n",
      "\n",
      "Cluster 4\n",
      "\n",
      "skill\n",
      "solution\n",
      "client\n",
      "analysis\n",
      "analytics\n",
      "include\n",
      "service\n",
      "system\n",
      "support\n",
      "management\n",
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "Topic 0:\n",
      "model learn product solution machine skill client analysis new company\n",
      "Topic 1:\n",
      "system position energy application research include model computer must require\n",
      "Topic 2:\n",
      "support skill include database management analysis service design project provide\n",
      "Topic 3:\n",
      "engineering development learn analytics state ai client software solution law\n",
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8891/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh-mantovani/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n",
      "127.0.0.1 - - [06/Feb/2020 12:25:38] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2020 12:25:38] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2020 12:25:39] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2020 12:25:39] \"GET /LDAvis.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Feb/2020 12:25:39] code 404, message Not Found\n",
      "127.0.0.1 - - [06/Feb/2020 12:25:39] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stopping Server...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Reading in data\n",
    "    df = pd.read_csv('../Datasets/df_all_linkedin.csv', index_col=0)\n",
    "    df_co = pd.read_csv('../Datasets/df_linkedin_Colorado.csv', index_col=0)\n",
    "\n",
    "    descriptions = df['Description'].values\n",
    "    descriptions_co = df_co['Description'].values\n",
    "\n",
    "    # Creating stop words\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    add_stopwords = {\n",
    "        'join', 'work', 'team', 'future', 'digital', 'technology', 'access', 'leader', 'industry', 'history', 'innovation',\n",
    "        'year', 'customer', 'focused', 'leading', 'business', 'ability', 'country', 'employee', 'www', 'seeking',\n",
    "        'location', 'role', 'responsible', 'designing', 'code', 'ideal', 'candidate', 'also', 'duty', 'without', 'excellent',\n",
    "        'set', 'area', 'well', 'use', 'strong', 'self', 'help', 'diverse', 'every', 'day', 'equal', 'employment', 'opportunity',\n",
    "        'affirmative', 'action', 'employer', 'diversity', 'qualified', 'applicant', 'receive', 'consideration', 'regard',\n",
    "        'race', 'color', 'religion', 'sex', 'national', 'origin', 'status', 'age', 'sexual', 'orientation', 'gender',\n",
    "        'identity', 'disability', 'marital', 'family', 'medical', 'protected', 'veteran', 'reasonable', 'accomodation',\n",
    "        'protect', 'status', 'equal', 'discriminate', 'inclusive', 'diverse'\n",
    "    }\n",
    "    stopWords = stopWords.union(add_stopwords)\n",
    "\n",
    "    # Initializing punctuation remover and lemmatizer\n",
    "    tokenize_remove_punct = RegexpTokenizer(r'\\w+')\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    # Cleaning descriptions for both the whole dataset and CO only\n",
    "    cleaned_descriptions = clean_descriptions(stopWords, descriptions_co)\n",
    "\n",
    "    # descriptions_no_sw_co = remove_stopwords(stopWords, descriptions_co)\n",
    "    # descriptions_no_sw_punct_co = remove_punctuation(tokenize_remove_punct, descriptions_no_sw_co)\n",
    "    # cleaned_descriptions_co = lemmatize_descriptions(lemma, descriptions_no_sw_punct_co)\n",
    "\n",
    "    # Vectorizing words creating both tf and tf-idf matrices\n",
    "    vectorizer = CountVectorizer(stop_words=stopWords, min_df=.15, max_df=0.75, max_features=5000)\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopWords, min_df=.15, max_df=0.75, max_features=5000)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(cleaned_descriptions).toarray()\n",
    "    tf = vectorizer.fit_transform(cleaned_descriptions)\n",
    "\n",
    "    # Initializing and fitting k-means model\n",
    "    kmeans = KMeans(n_clusters=5, verbose=True, n_jobs=-1)\n",
    "    kmeans.fit(tfidf)\n",
    "\n",
    "    # Returning most representative words for each cluster\n",
    "    get_representative_words(tfidf_vectorizer, kmeans)\n",
    "\n",
    "    # Calculating model score for kmeans\n",
    "    silhouette_score(tfidf, kmeans.labels_)\n",
    "    kmeans.score(tfidf)\n",
    "\n",
    "    # Initializing and running LDA model\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=4, \n",
    "                                    max_iter=10, learning_method='online', \n",
    "                                    random_state=0, verbose=True, n_jobs=-1)\n",
    "\n",
    "    lda.fit(tf)\n",
    "\n",
    "    # Displaying most representative words for each cluster of LDA\n",
    "    num_top_words=10\n",
    "    display_topics(lda, feature_names, num_top_words)\n",
    "\n",
    "    # LDA in gensim\n",
    "    # Processing text with gensim\n",
    "    data_text = df[['Description']].copy()\n",
    "    data_text['index'] = data_text.index\n",
    "    documents = data_text\n",
    "\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    processed_docs = documents['Description'].map(preprocess)\n",
    "\n",
    "    # Vectorizing text in gensim\n",
    "    id2word = gensim.corpora.Dictionary(processed_docs)\n",
    "    id2word.filter_extremes(no_below=80, no_above=.75, keep_n=5000)\n",
    "    texts = processed_docs\n",
    "    bow_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=3, id2word=id2word, passes=10, random_state=0)\n",
    "\n",
    "    # Visualizing LDA with PyLDAvis\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, id2word)\n",
    "    pyLDAvis.show(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
